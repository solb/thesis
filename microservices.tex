\chapter{Microsecond-scale microservices}
\label{chap:microservices}

\ifdefined\chapquotes
\begin{chapquote}{Douglas Adams, \textit{The Hitchhiker's Guide to the Galaxy}}
The ships hung in the sky in much the same way that bricks don't.
\end{chapquote}
\fi

This chapter provides a case study of how lightweight preemptible functions could be
used to create the serverless platform of the future.  We have not prototyped any of
its ideas on \textit{libinger}; rather, they formed the initial motivation for
fine-grained preemption and the inspiration for the preemptible function abstraction.

\begin{namespacereferences}{microservices:}
\input[microservices]{abstract}


\input[microservices]{intro}


\input[microservices]{motivation}
\end{namespacereferences}
\hspace{-2em}
Section~\ref{sec:libinger:quantum} demonstrates that interval timers are capable of
delivering signals with this frequency.


\begin{namespacereferences}{microservices:}
\input[microservices]{isolation}
\end{namespacereferences}

Given our performance goals, there is a crucial isolation aspect that Rust does not
provide:\@ there is nothing to stop users from monopolizing the CPU.  Our system,
however, must be preemptive.  We can apply preemptible functions
(Chapter~\ref{chap:functions}) here to impose a time quota on each microservice.
This has the added benefit of automatically providing memory isolation between
library dependencies (Chapter~\ref{chap:libgotcha}), insulating unsafe platform code
from affecting the state of other microservices or the rest of the worker process.

\begin{namespacereferences}{microservices:}
\input[microservices]{isolation_depth}


\input[microservices]{eval}


\input[microservices]{future}
\input[microservices]{future_rest}


\input[microservices]{concl}
\end{namespacereferences}

Since we published this benchmark, Cloudflare has built and deployed a production
FaaS platform called Workers.  Like our proposed architecture, this system omits
containers and virtual machines by running user code in
process~\cite{www-cloudflare-workers}.  It accomplishes this by requiring users to
submit JavaScript code (or WebAssembly) and running each task as a separate Isolate
under the V8 JavaScript engine~\cite{www-javascript-v8}, thereby halving major
competitors' cold-start latencies.  While running under a JavaScript engine confers
some practical benefits such as not having to audit dependencies, we believe that a
shift to native code will be necessary to further reduce cold-start latencies from
the millisecond range to the tens or hundreds of microseconds.
