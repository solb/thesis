\section{Deployment}
\label{sec:deploy}

We now describe our microservices in the broader context of our full proposed
serverless system.  We clarify their lifecycle, interactions with the compute nodes,
and the trust model for the cloud provider.

Users submit their microservices in the form of Rust source code, allowing the
serverless operator to pass the \texttt{-Funsafe-code} compilation flag to reject
any \texttt{unsafe} code.  This process need not occur on the compute
nodes, provided the deployment server tasked with compilation runs the same version
of the Rust compiler.\footnote{This restriction exists because, as of the latest
release (1.23.0) of the compiler, Rust does not have a stable ABI.}  The operator
needs to trust the compiler, standard library, and any libraries against which it
will permit the microservice to link (since they might contain \texttt{unsafe} code),
but importantly need not worry about the microservice itself.

We believe that restricting microservices to a specific
list of permitted dependencies is reasonable.  Any library that contains only
safe Rust code could be \whitelist{ed} without review.  To approximate the size of such
a list given the current Rust ecosystem, we turn to a 2017
study~\cite{www-cratesio-unsafe} by the Tock authors that found just under half of
the Rust package manager's top 1000 most-downloaded libraries to be free of
unsafe code.  They caution that many of those packages have unsafe
dependencies, but reviewing a relatively small number of popular
libraries would open up the majority of the most popular packages.

If the application compiles (is proven memory-safe) and links (depends only on
trusted libraries) successfully, the deployment server produces a shared object file,
which the provider then distributes to each compute node on which it might run.
Then, in order to ensure that invokers will experience the warm-start latencies
discussed in Section~\ref{sec:motive}, those nodes' dispatcher processes should
instruct one or more of their workers to preload the dynamic library.  If the
provider experiences too many active microservices for its available resources, it
can unload some libraries; on their next invocation, they will experience higher
(cold start) invocation latencies as they synchronously load the dynamic library.
