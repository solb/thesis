\section{Introduction}
\label{sec:intro}

As global Internet traffic has continued to grow in both quantity and variety, system designers have repeatedly had to reach for backends capable of handling ever-growing load.
Services that outgrew self-hosted servers were moved to datacenter racks, then eventually to virtualized cloud hosting environments.
However, this model failed to deliver two important and related benefits for users:
\begin{enumerate}
\item \textbf{Pay for only what you use at very fine granularity.} In the case of services that received only short, infrequent triggers, the VM approach still required users to pay continuously to keep an often-idle instance online and ready.
\item \textbf{Scale up rapidly on demand.} Faced with the prospect of highly-variable load, VM users had to pay to provision for the expected peak load, at a cost potentially orders of magnitude higher than that of the ordinarily necessary capacity [CITATION NEEDED].
\end{enumerate}

It was these shortcomings that helped encourage cloud providers to introduce a new model known as serverless computing, in which the customer provides \textit{only} their code, without having to configure its environment.
Recently, the largest players in the cloud space have introduced a form of this offering called function as a service (FaaS), marketing their implementations as AWS Lambda, Google Cloud Functions, Azure Functions, and Apache OpenWhisk [CITATION NEEDED].
The user code targeting these platforms has three properties:\ (1) it is \textbf{triggered by some event} (e.g., a storage event or an HTTP request to the provider's API), (2) it has a \textbf{capped runtime quota} and will be killed if attempts to run for longer, and (3) it runs within a container where it \textbf{may execute arbitrary code} subject to certain restrictions imposed to ensure isolation.
Importantly, it's the triggering event that causes a microservice to be started, and customers are only charged when it's running; furthermore, the platform is capable of automatically spawning many instances of the microservice to handle load spikes transparently.
Typical invocation latencies can be in the 100s of milliseconds for cold functions [CITATION NEEDED].
At the time of writing, AWS Lambda and Azure Functions currently caps instance execution time at 5 minutes, whereas Google Cloud Functions allows code to run for 9.

Cloud providers also tout another advantage of microservices:\ although they may not run for long, they do so very near to the datacenter's other services, increasing the amount of useful work that can be accomplished in that time.
Throughout the rise of this new cloud offering, datacenter network speeds have been steadily improving.
It now takes an average of only 10--20 $\mu$s round trip to send an RPC between two Azure machines on the same rack (?) [CITATION NEEDED].
We believe that in order to fully capitalize on these datacenter latencies, microservice invocation latencies must be brought down to meet them; for instance, only in this way can user experiences with such technologies as cloud-managed IoT devices be made truly seamless [ExCamera (Fouladi17) saw Lambda latencies in the seconds from their endpoint].
\mk{isn't satisfied with this claim}
Furthermore, setting such an aggressive goal will ensure that future applications enjoy good performance, and we suspect that such incredible invocation speed will enable new use cases of which we can't yet conceive.
In the words of Lampson, it's time to "make it fast rather than general or powerful." [CITE Hints for Computer System Design]

Setting a latency goal in the 10s of $\mu$s forces us to break from the design of traditional FaaS systems in an important way:\ spawning traditional VMs or even processes is now too slow [NEED BENCHMARKS IN A SUBSEQUENT SECTION].
To address this problem, we propose combining multiple user-submitted jobs into a single process that is pinned to a hardware CPU core.
In order for this approach to be practical, however, we must recover two properties we have given up:\ \textbf{isolation} and \textbf{fine-grained resource limits}.

In fact, recent years have seen a renewed interest in statically enforcing isolation and memory safety via a type-safe compiler.
One such project, \textit{Taking the V out of NFV} [CITE], shares our willingness to run users' code in-process within its network functions runtime.
We restore the isolation property by adopting their approach of requiring users to submit their jobs in the form of memory-safe programs written in the Rust programming language.

Given our performance goals, the most important resource limit to enforce is CPU time.
We argue that our system requires preemptive scheduling; otherwise, a single misbehaving user job can take control of its core.
In such a case, it can drive up the tail latency experienced by other jobs and even completely starve the other tasks that share its core.
This concern reinforces the decision to use Rust rather than a language such as Go or Erlang:\ although both languages have more evolved support for green threading, their models are nonpreemptive [CITATION NEEDED].
While there has been past work on preempting user threads, much of it has focused on design changes to the kernel that never became commonplace [CITE e.g. Scheduler Activations].
Though we could have implemented such kernel/user cooperation for this project, we worried that kernel changes would make our approach unattractive for real deployment, a fate that has befallen other systems with similar dependencies [CITE AFS?].
Fortunately, we find that modern CPUs' high precision event timers (HPETs) are sufficiently precise to allow high-frequency preemption in the 100s of $\mu$s \solb{refine this} without modifying the kernel; we present preliminary results demonstrating this in section TODO.

\solb{Do we want to discuss user runtime \textit{limits} here?}

In summary, in order to achieve average microservice invocation latencies in the 10s of $\mu$s, we propose a design that combines language-provided memory safety with fine-grained preemption.
