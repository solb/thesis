\section{Introduction}
\label{sec:intro}

As the scope and scale of Internet services continues to grow, system designers
have sought platforms that ease their scaling and deployment burden.
Services that outgrew self-hosted servers moved to datacenter racks, then eventually to virtualized cloud hosting environments.
However, this model only partially delivered two related benefits:
\begin{enumerate}
\item \textbf{Pay for only what you use at very fine granularity.}
\item \textbf{Scale up rapidly on demand.}
\end{enumerate}

The VM approach suffered from relatively coarse granularity, creating entire
machines at a time, and with minutes to months of minimum billing
time\footnote{Amazon recently followed Google's lead in moving down from
  hour-granuarlity, but both services remain more expensive for always-on VMs
  than providers who rent in month-long increments.}.  Relatively long startup
times required system designers to keep some spare capacity online to handle load
spikes.

These shortcoming led cloud providers to introduce a new model, known as
serverless computing, in which the customer provides \textit{only} their code,
without having to configure its environment.   Such ``Function as a Service''
(FaaS) platforms are now available as AWS Lambda, Google Cloud Functions, Azure
Functions, and Apache OpenWhisk.  These platforms provide a model in which:
(1)  User code is invoked upon some \textbf{event} occurring (e.g., an HTTP
request to some API), runs to completion, and nominally stops running (and being
billed) after it completes; and (2)  There is no state preserved between
separate invocations of the user code.  Property (2) enables easy auto-scaling
of the function as load changes.

Because these services execute within a cloud provider's existing
infrastructure, they reap the benefits of low-latency access to other cloud
services.  In fact, acting as an access-control proxy is a common use of a
microservice:  Receive an API request from a user, validate it, and then access
a back-end storage service, such as S3, using the service's credentials.

In this paper, we explore a design intended to reduce the tension between two of
the desiderata for cloud functions:  Low latency invocation and low cost.  In
today's practice, cloud functions execute within a heavyweight container, and
``cold start'' latencies range from hundreds of milliseconds to seconds.  (A
cold start occurs when the service has not received any requests for several
minutes).  To reduce cold starts, FaaS providers typically keep containers
around for several minutes after running a function, but doing so increases
costs.  Finally, of course, existing services already have latencies in the
millisecond range, as their requests traverse conventional OS stacks, go through
containers, etc.

We hypothesize---admittedly without much proof for this chicken-and-egg
scenario---that substantially reducing both the latency and cost of running
infrequently-used services will enable new classes and scales of applications
for cloud functions, and in the remainder of this paper, present a design that
achieves this.

\subsection{Not yet rewritten}

At the time of writing, AWS Lambda and Azure Functions currently caps instance execution time at 5 minutes, whereas Google Cloud Functions allows code to run for 9.

Cloud providers also tout another advantage of microservices:\ although they may not run for long, they do so very near to the datacenter's other services, increasing the amount of useful work that can be accomplished in that time.
Throughout the rise of this new cloud offering, datacenter network speeds have been steadily improving:
It now takes $<20\mu{}s$ for an RPC between two machines in Microsoft's Azure cloud~\cite{www-firestone-azure-latency}.
We believe that in order to fully capitalize on these datacenter latencies, microservice invocation latencies must be brought down to meet them; for instance, only in this way can user experiences with such technologies as cloud-managed IoT devices be made truly seamless [ExCamera (Fouladi17) saw Lambda latencies in the seconds from their endpoint].
\mk{isn't satisfied with this claim}
Furthermore, setting such an aggressive goal will ensure that future applications enjoy good performance, and we suspect that such incredible invocation speed will enable new use cases of which we can't yet conceive.
In the words of Lampson, it's time to "make it fast rather than general or powerful." [CITE Hints for Computer System Design]

Setting a latency goal in the 10s of $\mu$s forces us to break from the design of traditional FaaS systems in an important way:\ spawning traditional VMs or even processes is now too slow [NEED BENCHMARKS IN A SUBSEQUENT SECTION].
To address this problem, we propose combining multiple user-submitted jobs into a single process that is pinned to a hardware CPU core.
In order for this approach to be practical, however, we must recover two properties we have given up:\ \textbf{isolation} and \textbf{fine-grained resource limits}.

In fact, recent years have seen a renewed interest in statically enforcing isolation and memory safety via a type-safe compiler.
One such project, \textit{Taking the V out of NFV} [CITE], shares our willingness to run users' code in-process within its network functions runtime.
We restore the isolation property by adopting their approach of requiring users to submit their jobs in the form of memory-safe programs written in the Rust programming language.

Given our performance goals, the most important resource limit to enforce is CPU time.
We argue that our system requires preemptive scheduling; otherwise, a single misbehaving user job can take control of its core.
In such a case, it can drive up the tail latency experienced by other jobs and even completely starve the other tasks that share its core.
This concern reinforces the decision to use Rust rather than a language such as Go or Erlang:\ although both languages have more evolved support for green threading, their models are nonpreemptive [CITATION NEEDED].
While there has been past work on preempting user threads, much of it has focused on design changes to the kernel that never became commonplace [CITE e.g. Scheduler Activations].
Though we could have implemented such kernel/user cooperation for this project, we worried that kernel changes would make our approach unattractive for real deployment, a fate that has befallen other systems with similar dependencies [CITE AFS?].
Fortunately, we find that modern CPUs' high precision event timers (HPETs) are sufficiently precise to allow high-frequency preemption in the 100s of $\mu$s \solb{refine this} without modifying the kernel; we present preliminary results demonstrating this in section TODO.

\solb{Do we want to discuss user runtime \textit{limits} here?}

In summary, in order to achieve average microservice invocation latencies in the 10s of $\mu$s, we propose a design that combines language-provided memory safety with fine-grained preemption.
