\section{Introduction}
\label{sec:intro}

As the scope and scale of Internet services continues to grow, system designers
have sought platforms that simplify scaling and deployment.
Services that outgrew self-hosted servers moved to datacenter racks, then
eventually to virtualized cloud hosting environments.
However, this model only partially delivered two related benefits:
\begin{enumerate}
\item Pay for only what you use at very fine granularity.
\item Scale up rapidly on demand.
\end{enumerate}

The VM approach suffered from relatively coarse granularity, creating entire
machines at a time, and with minutes to months of minimum billing
time\footnote{Amazon recently followed Google's lead in moving down from
  hour-granularity, but both services remain more expensive for always-on VMs
  than providers who rent in month-long increments.}. \ak{I don't think we
need this footnote.} Relatively long startup
times required system designers to keep some spare capacity online to handle load
spikes.

These shortcomings led cloud providers to introduce a new model, known as
serverless computing, in which the customer provides \textit{only} their code,
without having to configure its environment.   Such ``Function as a Service''
(FaaS) platforms are now available as AWS Lambda~\cite{www-amazon-lambda}, Google
Cloud Functions~\cite{www-google-cf}, Azure Functions~\cite{www-microsoft-af}, and
Apache OpenWhisk~\cite{www-apache-openwhisk}.  These platforms provide a model in
which: (1)  User code is invoked whenever some \textbf{event} occurs (e.g., an HTTP
request to some API), runs to completion, and nominally stops running (and being
billed) after it completes; and (2)  There is no state preserved between
separate invocations of the user code.  Property (2) enables easy auto-scaling
of the function as load changes.

\solb{We used to address the 5- to 9-minute capped runtime quota here; add it back in
if we end up referring back to it.}

Because these services execute within a cloud provider's existing
infrastructure, they reap the benefits of low-latency access to other cloud
services.  In fact, acting as an access-control proxy is a recurring microservice
pattern:\ receive an API request from a user, validate it, then access
a backend storage service (e.g., S3) using the service's credentials.

In this paper, we explore a design intended to reduce the tension between two of
the desiderata for cloud functions:\ low latency invocation and low cost.  In
today's practice, cloud functions execute within a heavyweight container, and
``cold start'' latencies range from hundreds of milliseconds to seconds.  (A
cold start occurs when the service has not received any requests for several
minutes.)  To reduce cold starts, FaaS providers typically keep containers
around for several minutes after running a function, but doing so increases
costs.  Finally, of course, existing services already have latencies in the
millisecond range, as their requests traverse conventional OS stacks, go through
containers, etc.
\mk{Don't like the etc.}

\mk{The transition to this paragraph isn't clear.  Is the idea that we just said
low latency is important, but so is low tail latency?  The topic sentence should
probably say that then.}
Many modern distributed systems (e.g., those that handle search indexing or
assembling multi-component views for website front pages) involve
high-fanout communication, sometimes performing tens or thousands of
lookups to handle each user request.  Because their user-visible response time often
depends on the tail latency of the slowest chain of dependent
responses~\cite{Dean:cacm2013}, much recent work seeks to reduce
such systems' response
times~\cite{Jalaparti:sigcomm2013,Xu:nsdi2013,Li:socc2014,Jeon:asplos2016}.

This goal is supported by the impressively low network latencies available in
modern datacenters.  For example, it
now takes $<20\mu{}s$ to perform an RPC between two machines in Microsoft's Azure
cloud~\cite{www-firestone-azure-latency}.  We believe, however, that fully leveraging
this improving network performance will require reducing microservices' invocation
latencies to the point where the network is once again the bottleneck.

We further hypothesize---admittedly without much proof for this chicken-and-egg
scenario---that substantially reducing both the latency and cost of running
infrequently-used services will enable new classes and scales of applications
for cloud functions, and in the remainder of this paper, present a design that
achieves this.  As Lampson noted, there is power in making systems 
``fast rather than general or powerful''~\cite{Lampson1983}, because fast
building blocks can be used more widely.

\mk{Intro ends abuptly with an indication of what we are trying to do in this
paper: propose a new idea, a better design, preliminary results, etc.  The
previous text is mostly about the problem; do we have a solution/idea?}

In this paper, we propose a restructuring of the serverless model centered around
low-latency, \textit{lightweight microservices} run in \textit{shared processes}
and isolated primarily using \textit{compile-time guarantees} and
\textit{fine-grained preemption}.

%% \subsection{Not yet rewritten}

%% At the time of writing, AWS Lambda, Azure Functions, and Google Cloud Functions
%% cap instance execution time to 5--10 minutes.

%% \solb{AK: I see you touched this recently.  Do you still want to see us say it (and
%% where)?  I currently mention penultimate section of Motivation and related work that
%% providers presently impose a limit, without actually saying what it is.}
%% \mk{This subsection seems out of place.}
