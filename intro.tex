\section{Introduction}
\label{sec:intro}

As the scope and scale of Internet services continues to grow, system designers
have sought platforms that simplify scaling and deployment.
Services that outgrew self-hosted servers moved to datacenter racks, then
eventually to virtualized cloud hosting environments.
However, this model only partially delivered two related benefits:
\begin{enumerate}
\item Pay for only what you use at very fine granularity.
\item Scale up rapidly on demand.
\end{enumerate}

The VM approach suffered from relatively coarse granularity, creating entire
machines at a time, and with minutes to months of minimum billing
time\footnote{Amazon recently followed Google's lead in moving down from
  hour-granularity, but both services remain more expensive for always-on VMs
  than providers who rent in month-long increments.}. \ak{I don't think we
need this footnote.} Relatively long startup
times required system designers to keep some spare capacity online to handle load
spikes.

These shortcomings led cloud providers to introduce a new model, known as
serverless computing, in which the customer provides \textit{only} their code,
without having to configure its environment.   Such ``Function as a Service''
(FaaS) platforms are now available as AWS Lambda~\cite{www-amazon-lambda}, Google
Cloud Functions~\cite{www-google-cf}, Azure Functions~\cite{www-microsoft-af}, and
Apache OpenWhisk~\cite{www-apache-openwhisk}.  These platforms provide a model in
which: (1)  User code is invoked whenever some \textbf{event} occurs (e.g., an HTTP
request to some API), runs to completion, and nominally stops running (and being
billed) after it completes; and (2)  There is no state preserved between
separate invocations of the user code.  Property (2) enables easy auto-scaling
of the function as load changes.

\solb{We used to address the 5- to 9-minute capped runtime quota here; add it back in
if we end up referring back to it.}

Because these services execute within a cloud provider's existing
infrastructure, they reap the benefits of low-latency access to other cloud
services.  In fact, acting as an access-control proxy is a recurring microservice
pattern:\ receive an API request from a user, validate it, then access
a backend storage service (e.g., S3) using the service's credentials.

In this paper, we explore a design intended to reduce the tension between two of
the desiderata for cloud functions:\ low latency invocation and low cost.  In
today's practice, cloud functions execute within a heavyweight container, and
``cold start'' latencies range from hundreds of milliseconds to seconds.  (A
cold start occurs when the service has not received any requests for several
minutes.)  To reduce cold starts, FaaS providers typically keep containers
around for several minutes after running a function, but doing so increases
costs.  Finally, of course, existing services already have latencies in the
millisecond range, as their requests traverse conventional OS stacks, go through
containers, etc.

Many modern distributed systems (e.g., those that handle search indexing or
assembling multi-component views for high-traffic website front pages) involve
extremely high-fanout communication, sometimes performing tens or thousands of
lookups to handle each user request.  Because their user-visible response time often
depends on the tail latency of the slowest chain of dependent
responses~\cite{Dean:cacm2013}, much recent work has sought extreme reductions in
such systems' response
times~\cite{Jalaparti:sigcomm2013,Xu:nsdi2013,Li:socc2014,Jeon:asplos2016}.

The newest datacenters exhibit unprecedented low network latencies; for instance, it
now takes $<20\mu{}s$ to perform an RPC between two machines in Microsoft's Azure
cloud~\cite{www-firestone-azure-latency}.  We believe that fully leveraging
ever-decreasing network performance will require reducing microservices' invocation
latencies to the point where the network is once again the bottleneck.  As Lampson
put it, it's time to make serverless ``fast rather than general or
powerful~\cite{Lampson1983}.''

We also hypothesize---admittedly without much proof for this chicken-and-egg
scenario---that substantially reducing both the latency and cost of running
infrequently-used services will enable new classes and scales of applications
for cloud functions, and in the remainder of this paper, present a design that
achieves this.

\subsection{Not yet rewritten}

At the time of writing, AWS Lambda, Azure Functions, and Google Cloud Functions
cap instance execution time to 5--10 minutes.

\solb{AK: I see you touched this recently.  Do you still want to see us say it (and
where)?  I currently mention penultimate section of Motivation and related work that
providers presently impose a limit, without actually saying what it is.}
