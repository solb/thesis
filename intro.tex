\section{Introduction}
\label{sec:intro}

As the scope and scale of Internet services continues to grow, system designers
have sought platforms that simplify scaling and deployment.
Services that outgrew self-hosted servers moved to datacenter racks, then
eventually to virtualized cloud hosting environments.
However, this model only partially delivered two related benefits:
\begin{enumerate}
\item Pay for only what you use at very fine granularity.
\item Scale up rapidly on demand.
\end{enumerate}

The VM approach suffered from relatively coarse granularity:  Its atomic compute unit
of machines were billed at a minimum of minutes to months.  Relatively long startup
times required system designers to keep some spare capacity online to handle load
spikes.

These shortcomings led cloud providers to introduce a new model, known as
serverless computing, in which the customer provides \textit{only} their code,
without having to configure its environment.   Such ``function as a service''
(FaaS) platforms are now available as AWS Lambda~\cite{www-amazon-lambda}, Google
Cloud Functions~\cite{www-google-cf}, Azure Functions~\cite{www-microsoft-af}, and
Apache OpenWhisk~\cite{www-apache-openwhisk}.  These platforms provide a model in
which: (1)  User code is invoked whenever some event occurs (e.g., an HTTP
request to some API), runs to completion, and nominally stops running (and being
billed) after it completes; and (2)  There is no state preserved between
separate invocations of the user code.  Property (2) enables easy auto-scaling
of the function as load changes.

Because these services execute within a cloud provider's existing
infrastructure, they benefit from low-latency access to other cloud
services.  In fact, acting as an access-control proxy is a recurring microservice
pattern: receive an API request from a user, validate it, then access
a backend storage service (e.g., S3) using the service's credentials.
%\ak{Any references would be useful here.}

In this paper, we explore a design intended to reduce the tension between two of
the desiderata for cloud functions: low latency invocation and low cost.  Traditional
invocation techniques used today exhibit high latency with a
large tail; these latencies are
unsuitable for many modern distributed systems which involve
high-fanout communication, sometimes performing tens or thousands of
lookups to handle each user request.  Because their user-visible response time often
depends on the tail latency of the slowest chain of dependent
responses~\cite{Dean:cacm2013}, shrinking the tail is crucial~\cite{Jalaparti:sigcomm2013,
Xu:nsdi2013,Li:socc2014,Jeon:asplos2016}.
%\ak{The search indexing example is weird here.}

Thus we seek to reduce the invocation latency and improve predictability, a
goal supported by the impressively low network latencies available in
modern datacenters. It
now takes $<20\mu{}s$ to perform an RPC between two machines in Microsoft's Azure
cloud~\cite{www-firestone-azure-latency}.  \solb{Anuj mentioned there's a more recent
Google or Microsoft reference from NSDI.  I came across
\url{https://www.usenix.org/system/files/conference/nsdi17/nsdi17-firestone.pdf\#page=12}
(see right before Figure 10).  It gives a 25-$\mu{}s$ number that I \textit{think}
(?) is for between machines...}  We believe, however, that fully leveraging this
improving network performance will require reducing microservices' invocation
latencies to the point where the network is once again the bottleneck.

We further hypothesize---admittedly without much proof for this chicken-and-egg
scenario---that substantially reducing both the latency and cost of running
intermittently-used services will enable new classes and scales of applications
for cloud functions, and in the remainder of this paper, present a design that
achieves this.  As Lampson noted, there is power in making systems 
``fast rather than general or powerful''~\cite{Lampson1983}, because fast
building blocks can be used more widely.

Of course, a microservice will only ever be as fast as the slowest service it relies
on; however, recall that many such services are offered in the same clouds and
datacenters as serverless platforms.  It is our belief that decreasing network
latencies will push these services to respond faster as well, and that new hardware
technologies such as 3D XPoint will further accelerate this trend by offering
lower-latency reads \solb{I couldn't find a recent public citation to support this}.

In this paper, we propose a restructuring of the serverless model centered around
low-latency: \textit{lightweight microservices} run in \textit{shared processes}
and are isolated primarily using \textit{compile-time guarantees} and
\textit{fine-grained preemption}.

%% \subsection{Not yet rewritten}

%% At the time of writing, AWS Lambda, Azure Functions, and Google Cloud Functions
%% cap instance execution time to 5--10 minutes.

%% \solb{AK: I see you touched this recently.  Do you still want to see us say it (and
%% where)?  I currently mention penultimate section of Motivation and related work that
%% providers presently impose a limit, without actually saying what it is.}
%% \mk{This subsection seems out of place.}
