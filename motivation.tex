\section{Motivation}
\label{sec:motive}

Setting an invocation latency goal in the 10s of $\mu{}s$ based on the network
requires \textit{even lower} local invocation latencies on the compute node.  To
determine whether the design of traditional containerized FaaS systems is compatible
with this goal, we establish a lower bound on the latency achievable using
containers:\ we remove the containers altogether and model each microservice as a
separate process.  In this model, a cold start translates to a
\texttt{fork()/exec()}.  The large number of microservice processes means that our
warm start story cannot involve their remaining scheduled.  As such, we cause
inactive tasks to block and later wake them up with a Unix IPC mechanism; the
latencies of the available mechanisms being similar [\solb{CITE}], we opt for loopback UDP.
These two variants are shown in Figure~\ref{fig:motive} as the blue and green lines,
respectively.  Unfortunately, even the average warm start latency is high enough to
soon become a bottleneck on datacenter networks; worse, the 99.9\% tail latency
exceeds 10 $\mu{}s$, already slower than a network RPC.

From this we decide to eschew the separate processes for each microservice.  Instead,
we propose collecting groups of microservices into shared \textbf{worker processes},
each responsible for performing all the work assigned to a given CPU core.  We can
assume that these will always be running, so the cold start case now only requires us
to load user code into an existing process.  We do this by packaging user
microservices as shared object files and using the dynamic linker/loader's
\texttt{dlopen()} interface.  Because each worker owns its cores, they can spend any
(rare) time between user tasks polling for the next job to arrive, so a warm start
simply involves flipping a ready bit in memory shared between the two processes.
Both cases appear in Figure~\ref{fig:motive} as the orange and red lines,
respectively.  This time, the warm start is significantly faster than our objective
at just a few hundred \textit{nanoseconds}, with the 99.9\% tail below 2 $\mu{}s$.

\begin{figure}
\includegraphics[width=\columnwidth]{figs/2018-02-02-motivation_numfuns-latency-reordered}
\caption{Median one-way invocation latencies for a microservice that does no work (note the log scale)}
\label{fig:motive}
\end{figure}

We conducted this benchmark and all other experiments on one 16-core socket of a
Linux 4.13.0 equipped with 2.1-GHz Xeon E5-2683 Broadwell processors.  One physical
core was dedicated to a host-wide \textit{dispatch process} that spools requests and
forwards them to 14 physical cores dedicated to running the microservices; the
remaining core was reserved for use by a thread that would communicate with the wider
serverless cluster management system.

\solb{I'm planning to run this for various core counts as well, and to look at the memory usage.}
