\section{Motivation}
\label{sec:motive}

\begin{table}
\begin{center}
\small
\begin{tabular}{llrrr}
  \multicolumn{2}{c}{\textbf{Microservices}} & \multicolumn{2}{c}{\textbf{Latency (Âµs)}} & \textbf{Throughput} \\
  \textbf{Resident?} & \textbf{Isolation} & Median & 99\% & \textbf{(invocations/s)} \\
\midrule
\multirow{2}{*}{Warm-start} & Process & 8.7 & 27.3 & .29 million \\
& Language & 1.2 & 2.0 & 5.4 million \\
\midrule
\multirow{2}{*}{Cold-start} & Process & 2845.8 & 15976.0 & \multicolumn{1}{c}{---} \\
& Language & 38.7 & 42.2 & \multicolumn{1}{c}{---} \\
\end{tabular}
\caption{Microservice invocation performance}
\label{tab:invocperf}
\end{center}
\end{table}

Our decision to use language-based isolation is based on two experimental
findings:  (1) Process-level isolation is too slow for
microsecond-scale user functions. (2) High-precision timers on commodity
CPUs allow preemption at microsecond scale.  We conducted our experiments
on a single Intel Xeon E5-2683 v4 server (16 cores, 2.1~GHz) running
Linux 4.13.0; we discuss the results below.

\subsection{Process-level isolation is too slow}
We use a single-machine experiment to evaluate the invocation overhead of different
isolation techniques: We use 14 \emph{worker} CPU cores to run microservices. Another
core runs a \emph{dispatcher} process that initiates microservice execution on the
workers.  Since we only seek to model a single compute node, all requests originate
from the dispatcher, which in the case of a full serverless platform would instead
forward them from some cluster-level scheduler.  The dispatcher schedules up to 14
microservices at a time (i.e., one per worker core), choosing from a pool of 5,000
microservices.

To provide a comparison against contemporary system designs, we use two different
isolation mechanisms:
\begin{enumerate}
\item \textbf{Process-based isolation:} Each microservice is a separate process run
on one of the worker cores.
We expect this approach to exhibit lower latency than the container isolation common
in contemporary serverless deployments.
\item \textbf{Language-based isolation:} Each worker core runs a single-threaded
\emph{worker process} that is used to run different microservices, one at a time.
In this approach, shown in Figure~\ref{fig:sysdesign}, a worker process runs a
microservice by calling its registered
function; we assume that the microservice function can be isolated from the
worker process with language-based isolation techniques that we discuss in
Section~\ref{sec:isolation}. The dispatcher schedules microservices on worker
processes by sending them
requests on a shared memory queue, which worker processes poll to receive
new requests.
\end{enumerate}

\begin{figure}
\includegraphics[width=\columnwidth]{figs/system-crop}
\caption{Language-based isolation design.  The dispatcher process
uses shared in-memory queues to feed requests to the worker processes, each of
which runs one user-supplied microservice at a time.}
\label{fig:sysdesign}
\end{figure}

We use 5,000 copies of a trivial microservice that simply records a timestamp.
Latency measurements are from when the dispatcher process invokes a microservice
until the moment that microservice records its timestamp.  We run the experiment in
two modes:

\paragraph{Warm-start requests.}
We first model a situation where all of the microservices are already resident on the
compute node.  In the case of process-based isolation, the dispatcher launches all
5,000 microservices at the beginning of the experiment, but they all block on an IPC
call; the dispatcher then invokes each microservice by waking up its process using a
UDP datagram.  In the case of language-based isolation, the microservices are
preloaded into worker processes.

Table~\ref{tab:invocperf} shows the latency and throughput achieved by the two
methods.  We find that the process-based isolation approach takes \us{9} and achieves
only 300,000 microservice invocations per second. In contrast, language-based
isolation achieves \us{1.2} (with a tail of just \us{2.0}) and over 5 million
invocations per second.

A \us{9} delay in microservice invocation is already a substantial fraction of
Azure's \us{20} RPC latency, a fraction that will only increase with
improvements in network latency. We therefore conclude that even in the average case,
process-based isolation is too slow for microsecond-scale scheduling; furthermore,
invocation throughput is also limited by IPC overhead.

The total memory footprint also shrinks:\@ loading 5,000 resident
trivial microservices consumes 2 GiB of memory with the process-based approach, but
only 1.3 GiB with the language-based one.  However, this benefit may be reduced as
microservices' code sizes increase.

\paragraph{Cold-start requests.}
While achieving ideal wakeup times is only possible when the microservices are
already resident in the system, the tail latency of the serverless platform will be
determined by the time taken to process requests whose microservices must be loaded
before they can be invoked.  To assess the difference between process-based and
language-based isolation in this context, we run the experiment with the following
change:  In the process model, the dispatch thread now launches a microservice
process to handle each request by \texttt{fork()/exec()}'ing.  In the language model,
the dispatch thread asks a worker to load a microservice's dynamic library (and
unload it afterward).  The results, shown in in Table~\ref{tab:invocperf}, reveal an
order-of-magnitude slip in the language-based approach's latency; however, this is
overshadowed by the three orders of magnitude increase in the case of process-based
isolation.

\subsection{Intra-process preemption is fast}
With language-based isolation, we run user-provided microservice functions
directly in worker processes. To prevent rogue functions from endlessly blocking
other microservices, we must premept or terminate functions that run for too
long (e.g., longer than \us{100}).  This preemption interval is orders of
magnitude faster than Linux's default 4ms time quantum.

Fortunately, we found that high-precision event timers (HPETs) on modern CPUs
are sufficient for this task. We measure the granularity and reliability of
these timers as follows: We run one thread that requests a periodic timer with a
configurable interval of $T$~\textmu{}s and installs an associated signal handler.
Ideally, this handler would always be called exactly $T$~\textmu{}s after its last
invocation; we measure the deviation from $T$ over 256 iterations.
We find that the variance
is smaller than 0.3~\textmu{}s for $T \ge 3$~\textmu{}s. This shows that
intra-process preemption is fast and reliable enough for our needs.
